<!DOCTYPE html>
<html lang="en"><head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Xinyu Liu - Homepage</title>
    
    <meta name="author" content="Xinyu Liu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <link rel="stylesheet" type="text/css" href="files/stylesheet.css">
<script type="text/javascript" src="files/jquery.min.js"></script></head>

    <body>
        <table width="980" border="0" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <p align="center"><font size="7">Xinyu Liu | 刘昕煜 </font><br>
                           <!--  <a href="http://gvv.mpi-inf.mpg.de/">GVV Group, Max Planck Institute for Informatics, Germany</a><br> -->
                            <!-- Office: Room 3220<br>
                            A.V. Williams Bldg.<br>
                            8223 Paint Branch Dr.<br>
                            College Park, MD 20742<br> -->
<!--                             <a href="assets/CV_Xinyu_Liu.pdf">CV (Updated in Jan. 2023) </a> |   -->
                            <p style="text-align:center">
                  <a href="xliugd@connect.ust.hk">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=kgRjFN8AAAAJ&hl=zh-CN"><b>Google Scholar</b></a> &nbsp;/&nbsp;
                  <a href="https://github.com/Liuxinyv/"><b>GitHub</b></a>
              </p> </font>
                        </p>
                      <!--   <p align=center> <font size='5'>
                          
                            <img style="width: 15px; height:18px;" src="cv.jpg"> <a href="cv.pdf">CV / Resume</a> |
                            <img style="width: 18px; height:18px;" src="gscholar-button_inactive.svg">
                                <a href="https://scholar.google.com/citations?user=HvJj-pEAAAAJ&hl=en">Google Scholar</a> | 
                            <img style="width: 18px; height:18px;" src="github.png">
                                <a href="https://github.com/WilsonWangTHU/">Github</a> </font>
                        </p>
 -->
                        <tr>
                            <td width="50%" valign="middle" align="justify">


                             
                                I am a PhD student of the University of Science and Technology (HKUST) supervised by <a href="https://cse.hkust.edu.hk/admin/people/faculty/profile/yikeguo"> Prof. Yi-Ke Guo</a>. I received my master’s degree at Xidian University, advised by <a href="https://scholar.google.com/citations?user=FZbrL2YAAAAJ&hl=en">Prof. Licheng Jiao</a>. Additionally, I worked as a research intern at <a href="https://air.tsinghua.edu.cn/">Institute for AI Industry Research (AIR)</a>, Tsinghua University, advised by <a href="https://sites.google.com/view/fromandto/"> assistant prof. Hao Zhao</a>. I got my bachelor’s degree in June 2020 from the Department of Communication Engineering, Xidian University.</p>


                                My research interests lie in the area of video generation and 3D Diffusion Models.<br> 
                            </td>

                           <td style="padding:2.5%;width:30%;max-width:40%">
                <a href="assets/xinyu.jpg"><img style="width:75%;max-width:100%" alt="profile photo" src="assets/xinyu.jpg" class="hoverZoomLink"></a>
              </td>
                                
                            </td>
                        </tr>
                    </table>


    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Xinyu Liu (刘昕煜)</name>
                </p>
                <p style="text-align:center">
                  <a href="xliugd@connect.ust.hk"><b>Email</b></a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=kgRjFN8AAAAJ&hl=zh-CN"><b>Google Scholar</b></a> &nbsp;/&nbsp;
                  <a href="https://github.com/Liuxinyv/"><b>GitHub</b></a>
              </p>
                <p style="text-align:justify;line-height:180%">
                  I am a PhD student of the University of Science and Technology (HKUST) supervised by <a href="https://cse.hkust.edu.hk/admin/people/faculty/profile/yikeguo"> Prof. Yi-Ke Guo</a>. I received my master’s degree at Xidian University, advised by <a href="https://scholar.google.com/citations?user=FZbrL2YAAAAJ&hl=en">Prof. Licheng Jiao</a>. Additionally, I worked as a research intern at <a href="https://air.tsinghua.edu.cn/">Institute for AI Industry Research (AIR)</a>, Tsinghua University, advised by <a href="https://sites.google.com/view/fromandto/"> assistant prof. Hao Zhao</a>. I got my bachelor’s degree in June 2020 from the Department of Communication Engineering, Xidian University.</p>
		            </p>

              </td>
              <td style="padding:2.5%;width:48%;max-width:48%">
                <img style="width:80%;max-width:80%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="assets/liuxinyu.png" class="hoverZoomLink">
              </td>
	      </tr>
	      </tbody></table> 

</br>

   <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <h2>Publications</h2>
        </td>
      </tr>
      </tbody>
    </table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
              <td style="padding:14px;width:45%;max-width:45%" align="center">
                  <img style="width:100%;max-width:100%" src="assets/hiprompt.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <a href="https://github.com/Liuxinyv/HiPrompt"><papertitle>HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts</papertitle></a>
                <br>
                  <strong>Xinyu Liu</strong>, Yingqing He, Lanqing Guo, Xiang Li, Bu Jin, Peng Li, Yan Li, Chi-Min Chan, Qifeng Chen, Wei Xue, Wenhan Luo, Qingfeng Liu, QiYike Guo
                <br>

                <a href="https://liuxinyv.github.io/HiPrompt/"><b>[Homepage]</b></a>
                <a href="https://arxiv.org/abs/2409.02919"><b>[arXiv]</b></a>
                <a href="https://github.com/Liuxinyv/HiPrompt"><b>[Code]</b></a>
                <a href="https://github.com/Liuxinyv/HiPrompt"><img src="https://img.shields.io/github/stars/Liuxinyv/HiPrompt.svg?style=social&label=Star&maxAge=2592000"></a>
                <br>
                <!-- <p>We introduced the new task of outdoor 3D dense captioning with TOD3Cap dataset; We proposed TOD3Cap network, leveraging the BEV representation to encode sparse outdoor scenes, and combine Relation Q-Former with LLaMA-Adapter to dense captioning in the open-world. </p>	 -->
              </td>
          </tr>

          <tr>
              <td style="padding:14px;width:45%;max-width:45%" align="center">
                  <img style="width:100%;max-width:100%" src="assets/tod3cap.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <a href="https://jxbbb.github.io/TOD3Cap"><papertitle>TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes</papertitle></a>
                <br>
                  Bu Jin,
                  Yupeng Zheng,
                  Pengfei Li,
                  Weize Li,
                  Yuhang Zheng,
                  Sujie Hu,
                <strong>Xinyu Liu</strong>,
                  Jinwei Zhu,
                  Zhijie Yan,
                  Haiyang Sun,
                  Kun Zhan,
                  Peng Jia,
                  Xiaoxiao Long,
                  Yilun Chen,
                  Hao Zhao
                <br>
                <strong><em>ECCV, 2024</em></strong>
                <br>
                <a href="https://jxbbb.github.io/TOD3Cap"><b>[Homepage]</b></a>
                <a href="https://arxiv.org/abs/2403.19589"><b>[arXiv]</b></a>
                <a href="https://github.com/jxbbb/TOD3Cap"><b>[Code]</b></a>
                <a href="https://github.com/jxbbb/TOD3Cap"><img src="https://img.shields.io/github/stars/jxbbb/TOD3Cap.svg?style=social&label=Star&maxAge=2592000"></a>
                <br>
                <!-- <p>We introduced the new task of outdoor 3D dense captioning with TOD3Cap dataset; We proposed TOD3Cap network, leveraging the BEV representation to encode sparse outdoor scenes, and combine Relation Q-Former with LLaMA-Adapter to dense captioning in the open-world. </p>	 -->
              </td>
          </tr>
	    
		<tr>
              <td style="padding:14px;width:45%;max-width:45%" align="center">
                  <img style="width:100%;max-width:100%" src="assets/SAZS.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Delving_Into_Shape-Aware_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf"><papertitle>Delving into Shape-aware Zero-shot Semantic Segmentation</papertitle></a>
                <br>
                <strong>Xinyu liu</strong>, 
                  Beiwen Tian, 
                  Yupeng Zheng, 
                  Zhen Wang, 
		  Rui Wang,
		  Kehua Sheng, 
		  Bo Zhang,
                  Hao Zhao, 
                  Guyue Zhou
                <br>
                <strong><em>CVPR, 2023 </em></strong>
                <br>
                <a href="https://arxiv.org/abs/2304.08491/"><b>[arXiv]</b></a>
                <a href="https://github.com/Liuxinyv/SAZS/"><b>[Code]</b></a>
                <a href="https://github.com/Liuxinyv/SAZS/"><img src="https://img.shields.io/github/stars/Liuxinyv/SAZS.svg?style=social&label=Star&maxAge=2592000"></a>

                <br>
                <!-- <p>We presented Adapt (Action-aware Driving cAPtion Transformer), a new end-to-end transformer-based framework for generating action narration and reasoning for self-driving vehicle. </p>	 -->
              </td>
            </tr>
            <tr>
              <td style="padding:14px;width:45%;max-width:45%" align="center">
                  <img style="width:100%;max-width:100%" src="assets/adapt.gif" alt="dise">
              </td>
              <td width="75%" valign="center">
                <a href="https://ieeexplore.ieee.org/abstract/document/10160326"><papertitle>Adapt: Action-aware Driving Caption Transformer</papertitle></a>
                <br>
                  Bu Jin, 
                <strong>Xinyu Liu</strong>, 
                  Yupeng Zheng, 
                  Pengfei Li, 
                  Hao Zhao<sup>†</sup>, 
                  Tong Zhang, 
                  Yuhang Zheng, 
                  Guyue Zhou, 
                  Jingjing Liu
                <br>
                <strong><em>ICRA, 2023 </em></strong>
                <br>
                <a href="https://arxiv.org/abs/2302.00673"><b>[arXiv]</b></a>
                <a href="https://github.com/jxbbb/ADAPT"><b>[Code]</b></a>
                <a href="https://github.com/jxbbb/ADAPT"><img src="https://img.shields.io/github/stars/jxbbb/ADAPT.svg?style=social&label=Star&maxAge=2592000"></a>

                <br>
                <!-- <p>We presented Adapt (Action-aware Driving cAPtion Transformer), a new end-to-end transformer-based framework for generating action narration and reasoning for self-driving vehicle. </p>	 -->
              </td>
            </tr>


                    
    </body>
</html
